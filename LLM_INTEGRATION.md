# ü§ñ Integra√ß√£o com LLMs (Groq, OpenAI, Claude)

Este documento mostra como usar a API com Large Language Models para extra√ß√£o avan√ßada de dados.

## üéØ Endpoint Espec√≠fico para LLM

A API possui o endpoint `/extract-for-llm` que prepara o texto otimizado para consumo por LLMs.

### Diferen√ßa entre os endpoints

| Endpoint | Uso | Retorno |
|----------|-----|---------|
| `/extract` | Extra√ß√£o tradicional com regex | Dados estruturados |
| `/extract-for-llm` | Texto preparado para LLM | Texto + prompt + dados estruturados |

## üöÄ Como Funciona

```
PDF ‚Üí API ‚Üí Texto Limpo + Prompt Otimizado ‚Üí LLM ‚Üí Dados Avan√ßados
```

## üìù Exemplo de Requisi√ß√£o

```python
import requests

# 1. Envie o PDF para o endpoint LLM
with open("fatura.pdf", "rb") as f:
    response = requests.post(
        "http://localhost:8000/extract-for-llm",
        files={"file": f}
    )

data = response.json()

# 2. Extraia os dados preparados
llm_data = data["llm_prompt_data"]
system_instruction = llm_data["system_instruction"]
document_content = llm_data["document_content"]
suggested_prompt = llm_data["suggested_prompt"]

print(f"Texto tem {llm_data['document_stats']['total_words']} palavras")
```

## üåü Integra√ß√£o com Groq

### Setup

```bash
pip install groq
```

### C√≥digo Completo

```python
import requests
from groq import Groq

# 1. Extrai texto via API
with open("fatura.pdf", "rb") as f:
    response = requests.post(
        "http://localhost:8000/extract-for-llm",
        files={"file": f}
    )

api_data = response.json()
llm_data = api_data["llm_prompt_data"]

# 2. Envia para Groq
client = Groq(api_key="sua-chave-groq")

completion = client.chat.completions.create(
    model="mixtral-8x7b-32768",  # ou "llama3-70b-8192"
    messages=[
        {
            "role": "system",
            "content": llm_data["system_instruction"]
        },
        {
            "role": "user",
            "content": llm_data["suggested_prompt"]
        }
    ],
    temperature=0.1,  # Baixa temperatura para mais precis√£o
    max_tokens=2000
)

# 3. Processa resposta do LLM
llm_response = completion.choices[0].message.content
print("Resposta do LLM:")
print(llm_response)

# 4. Compare com extra√ß√£o tradicional
traditional = api_data["traditional_extraction"]
print(f"\nExtra√ß√£o Tradicional (confian√ßa: {traditional['confidence']}):")
print(f"Empresa: {traditional['data']['empresa']}")
print(f"Valor: R$ {traditional['data']['valor_total']}")
```

## üîì Integra√ß√£o com OpenAI

```python
import requests
from openai import OpenAI

# 1. Extrai via API
with open("documento.pdf", "rb") as f:
    response = requests.post(
        "http://localhost:8000/extract-for-llm",
        files={"file": f}
    )

api_data = response.json()
llm_data = api_data["llm_prompt_data"]

# 2. Usa OpenAI
client = OpenAI(api_key="sua-chave-openai")

completion = client.chat.completions.create(
    model="gpt-4-turbo-preview",  # ou "gpt-3.5-turbo"
    messages=[
        {
            "role": "system",
            "content": llm_data["system_instruction"]
        },
        {
            "role": "user",
            "content": llm_data["suggested_prompt"]
        }
    ],
    temperature=0.2,
    response_format={"type": "json_object"}  # For√ßa resposta em JSON
)

result = completion.choices[0].message.content
print(result)
```

## üß† Integra√ß√£o com Claude (Anthropic)

```python
import requests
import anthropic

# 1. Extrai via API
with open("nota_fiscal.pdf", "rb") as f:
    response = requests.post(
        "http://localhost:8000/extract-for-llm",
        files={"file": f}
    )

api_data = response.json()
llm_data = api_data["llm_prompt_data"]

# 2. Usa Claude
client = anthropic.Anthropic(api_key="sua-chave-anthropic")

message = client.messages.create(
    model="claude-3-opus-20240229",  # ou "claude-3-sonnet-20240229"
    max_tokens=2000,
    system=llm_data["system_instruction"],
    messages=[
        {
            "role": "user",
            "content": llm_data["suggested_prompt"]
        }
    ]
)

print(message.content[0].text)
```

## üí° Prompt Customizado

Voc√™ pode criar seu pr√≥prio prompt em vez de usar o sugerido:

```python
# Extrai dados da API
response = requests.post(
    "http://localhost:8000/extract-for-llm",
    files={"file": open("fatura.pdf", "rb")}
)

llm_data = response.json()["llm_prompt_data"]

# Prompt customizado para an√°lise espec√≠fica
custom_prompt = f"""
Analise este documento financeiro e responda em JSON:

1. Qual o tipo exato de documento?
2. Existe algum valor em atraso?
3. H√° multas ou juros aplicados?
4. Qual a data limite para pagamento?
5. Existem descontos para pagamento antecipado?

Documento:
{llm_data['document_content']}

Responda APENAS com JSON v√°lido seguindo esta estrutura:
{{
  "tipo_documento": "",
  "valor_atraso": 0.0,
  "multas_juros": 0.0,
  "data_limite": "",
  "desconto_antecipado": 0.0
}}
"""

# Use este prompt com seu LLM preferido
```

## üé® Casos de Uso Avan√ßados

### 1. Valida√ß√£o e Corre√ß√£o de Dados

```python
# Combina extra√ß√£o tradicional + LLM para valida√ß√£o
api_response = requests.post(
    "http://localhost:8000/extract-for-llm",
    files={"file": open("boleto.pdf", "rb")}
).json()

traditional = api_response["traditional_extraction"]["data"]
llm_data = api_response["llm_prompt_data"]

# Cria prompt de valida√ß√£o
validation_prompt = f"""
Os seguintes dados foram extra√≠dos automaticamente de um boleto:

Empresa: {traditional['empresa']}
CNPJ: {traditional['cnpj']}
Valor: R$ {traditional['valor_total']}
Vencimento: {traditional['data_vencimento']}

Baseado no texto completo do documento abaixo, valide se estes dados est√£o corretos
e corrija qualquer erro encontrado:

{llm_data['document_content']}

Responda em JSON com os dados validados/corrigidos.
"""

# Envia para LLM para valida√ß√£o
```

### 2. Extra√ß√£o de Dados N√£o Estruturados

```python
# Usa LLM para extrair informa√ß√µes que regex n√£o consegue
prompt = f"""
Do documento abaixo, extraia:
1. Todos os nomes de pessoas mencionados
2. Endere√ßos completos
3. Observa√ß√µes ou notas importantes
4. Condi√ß√µes especiais de pagamento
5. Qualquer informa√ß√£o sobre garantias ou seguros

{llm_data['document_content']}

Retorne em JSON estruturado.
"""
```

### 3. An√°lise Comparativa

```python
# Processa m√∫ltiplos documentos e compara
documentos = ["fatura_jan.pdf", "fatura_fev.pdf", "fatura_mar.pdf"]
extracted_data = []

for doc in documentos:
    response = requests.post(
        "http://localhost:8000/extract-for-llm",
        files={"file": open(doc, "rb")}
    ).json()
    extracted_data.append(response)

# Cria prompt de an√°lise comparativa
comparison_prompt = f"""
Compare estas 3 faturas mensais e identifique:
1. Tend√™ncia de aumento ou diminui√ß√£o de valores
2. Novos itens ou servi√ßos adicionados
3. Itens removidos
4. Varia√ß√£o percentual m√©dia
5. Alertas ou anomalias

Fatura Janeiro:
{extracted_data[0]['llm_prompt_data']['document_content'][:500]}...

Fatura Fevereiro:
{extracted_data[1]['llm_prompt_data']['document_content'][:500]}...

Fatura Mar√ßo:
{extracted_data[2]['llm_prompt_data']['document_content'][:500]}...
"""
```

## üìä Estrutura de Resposta do Endpoint

```json
{
  "success": true,
  "llm_prompt_data": {
    "system_instruction": "Voc√™ receber√° um documento financeiro...",
    "document_content": "Texto completo extra√≠do e limpo",
    "document_stats": {
      "total_lines": 150,
      "total_words": 1250,
      "total_chars": 8500,
      "has_sections": true
    },
    "extraction_metadata": {
      "pdf_type": "native",
      "extraction_method": "pdfplumber",
      "pages": 2
    },
    "structured_sections": {
      "header": "...",
      "dados do cliente": "...",
      "valores": "..."
    },
    "suggested_prompt": "Analise o seguinte documento..."
  },
  "traditional_extraction": {
    "document_type": "fatura_cartao",
    "confidence": 0.85,
    "data": {
      "empresa": "Banco Exemplo",
      "valor_total": 1500.00,
      ...
    }
  },
  "usage_example": {
    "description": "Use o 'suggested_prompt' com seu LLM favorito",
    "groq_example": "...",
    "openai_example": "..."
  }
}
```

## üî• Exemplo Completo: Pipeline H√≠brido

Combina extra√ß√£o tradicional (r√°pida) com LLM (precisa):

```python
import requests
from groq import Groq

def extract_with_hybrid_approach(pdf_path: str) -> dict:
    """
    Abordagem h√≠brida: usa extra√ß√£o tradicional primeiro,
    depois LLM apenas se confian√ßa for baixa.
    """
    
    # 1. Extra√ß√£o completa
    with open(pdf_path, "rb") as f:
        response = requests.post(
            "http://localhost:8000/extract-for-llm",
            files={"file": f}
        ).json()
    
    traditional = response["traditional_extraction"]
    confidence = traditional["confidence"]
    
    # 2. Se confian√ßa for alta, usa dados tradicionais
    if confidence > 0.8:
        print(f"‚úÖ Alta confian√ßa ({confidence}), usando extra√ß√£o tradicional")
        return traditional["data"]
    
    # 3. Se confian√ßa for baixa, usa LLM para melhorar
    print(f"‚ö†Ô∏è Baixa confian√ßa ({confidence}), usando LLM para refinar")
    
    llm_data = response["llm_prompt_data"]
    
    groq = Groq(api_key="sua-chave")
    completion = groq.chat.completions.create(
        model="mixtral-8x7b-32768",
        messages=[
            {"role": "system", "content": llm_data["system_instruction"]},
            {"role": "user", "content": llm_data["suggested_prompt"]}
        ],
        temperature=0.1
    )
    
    llm_result = completion.choices[0].message.content
    
    # 4. Combina resultados
    return {
        "traditional": traditional["data"],
        "llm_enhanced": llm_result,
        "method": "hybrid",
        "confidence": confidence
    }

# Uso
result = extract_with_hybrid_approach("fatura_complexa.pdf")
print(result)
```

## üí∞ Economia de Tokens

A API j√° faz pr√©-processamento e limpeza, economizando tokens do LLM:

| Sem API | Com API |
|---------|---------|
| ~5000 tokens | ~2000 tokens |
| Texto sujo com OCR bruto | Texto limpo e estruturado |
| M√∫ltiplas tentativas | Prompt otimizado |

**Economia estimada: 60% de tokens!**

## üéØ Melhores Pr√°ticas

1. **Use temperatura baixa** (0.1-0.3) para dados estruturados
2. **Especifique formato JSON** na resposta quando poss√≠vel
3. **Combine com extra√ß√£o tradicional** para valida√ß√£o
4. **Cache resultados** de documentos processados
5. **Use modelos adequados**: Mixtral/GPT-4 para precis√£o, GPT-3.5 para velocidade

## üîó Links √öteis

- [Groq API Docs](https://console.groq.com/docs)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Anthropic Claude Docs](https://docs.anthropic.com/)

---

**Dica:** Para documentos muito grandes, use o campo `structured_sections` do LLM data para processar se√ß√£o por se√ß√£o, economizando ainda mais tokens!
